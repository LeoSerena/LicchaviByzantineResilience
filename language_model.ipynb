{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-radiation",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "falling-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import typing\n",
    "import gc\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.join('..','src')\n",
    ")\n",
    "\n",
    "from src.models import NextWordPredictorModel\n",
    "from src.data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tropical-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-startup",
   "metadata": {},
   "source": [
    "## Global Variables & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brazilian-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "assert torch.cuda.is_available()\n",
    "from apex import amp, optimizers\n",
    "\n",
    "MIN_SEQ_LEN = 2\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_LSTM_LAYERS = 2\n",
    "EMB_SIZE = 128\n",
    "HIDDEN_STATE_SIZE = 100\n",
    "DROPOUT = 0.5\n",
    "fp16 = True\n",
    "POS_ENCODING = False\n",
    "STARTING_LR = 1e-3\n",
    "\n",
    "val_split = 0.2\n",
    "test_split = 0.1\n",
    "\n",
    "# for reproducibility\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-excuse",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "internal-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_text = ' '.join(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "juvenile-temple",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = FromRawTextVocabulary(\n",
    "    text = train_text,\n",
    "    tokenizer = tokenizer,\n",
    "    text_cleaner = None,\n",
    "    max_voc_size = 10000,\n",
    "    min_word_occ = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-barcelona",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominican-burton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "\n",
    "train_text = ' '.join(train_iter)\n",
    "val_text = ' '.join(val_iter)\n",
    "test_text = ' '.join(test_iter)\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    vocabulary = vocabulary,\n",
    "    text = train_text,\n",
    "    max_seq_length = MAX_SEQ_LEN + 1,\n",
    "    min_seq_length = MIN_SEQ_LEN,\n",
    "    device = DEVICE\n",
    ")\n",
    "val_dataset = SequenceDataset(\n",
    "    vocabulary = vocabulary,\n",
    "    text = val_text,\n",
    "    max_seq_length = MAX_SEQ_LEN + 1,\n",
    "    min_seq_length = MIN_SEQ_LEN,\n",
    "    device = DEVICE\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    vocabulary = vocabulary,\n",
    "    text = test_text,\n",
    "    max_seq_length = MAX_SEQ_LEN + 1,\n",
    "    min_seq_length = MIN_SEQ_LEN,\n",
    "    device = DEVICE\n",
    ")\n",
    "\n",
    "del train_text\n",
    "del val_text\n",
    "del test_text\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pleasant-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    pin_memory = False,\n",
    "    drop_last = True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    pin_memory = False,\n",
    "    drop_last = True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    pin_memory = False,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imposed-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weights(weights, m_ = 0.01, M_ = 1):\n",
    "    weights = 1 / weights\n",
    "    M, m = max(weights), min(weights)\n",
    "    return (np.array(weights) - m) * (M_ - m_) / (M - m) + m_\n",
    "\n",
    "weights = map_weights(np.array(list(vocabulary.vocab.values())))\n",
    "weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "willing-express",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "model = NextWordPredictorModel(\n",
    "    emb_dim  = EMB_SIZE,\n",
    "    vocab_size = vocabulary.get_vocab_size(),\n",
    "    num_lstm_hidden_layers = NUM_LSTM_LAYERS,\n",
    "    hidden_state_size = HIDDEN_STATE_SIZE,\n",
    "    dropout = DROPOUT,\n",
    "    device = DEVICE,\n",
    "    lr = STARTING_LR,\n",
    "    fp16 = fp16,\n",
    "    weight = weights,\n",
    "    positional_encoding = POS_ENCODING\n",
    ").to(DEVICE)\n",
    "\n",
    "if fp16:\n",
    "    model, model.optimizer = amp.initialize(\n",
    "        model,\n",
    "        model.optimizer,\n",
    "        opt_level = 'O1' # https://nvidia.github.io/apex/amp.html\n",
    "    )\n",
    "\n",
    "model.scheduler = torch.optim.lr_scheduler.StepLR(model.optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2084/6870 [00:06<00:15, 303.11it/s]"
     ]
    }
   ],
   "source": [
    "metrics = model.fit(\n",
    "    train_dataloader = train_dataloader,\n",
    "    eval_dataloader = val_dataloader,\n",
    "    num_epochs = 5,\n",
    "    early_stopping = True,\n",
    "    early_stopping_patience = 2,\n",
    "    early_stopping_metric = 'val_loss',\n",
    "    early_stopping_metric_best = 'min', # if lower is better (like for loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metrics).T\n",
    "plt.figure()\n",
    "plt.plot(df['train_loss'])\n",
    "plt.plot(df['val_loss'])\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Historians write in the context of their own time and with due regard to the current dominant ideas of how to interpret the past'\n",
    "ind = [train_dataset.get_idx(w.lower()) for w in sent.split(' ')]\n",
    "print(ind)\n",
    "hidden = model.init_hidden(1)\n",
    "inputs = torch.tensor([ind]).to(DEVICE)\n",
    "model.eval()\n",
    "output, _ = model(inputs, hidden)\n",
    "preds = output.view(-1, model.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "for top4 in preds.topk(4).indices:\n",
    "    res = []\n",
    "    for l in top4:\n",
    "        w = vocabulary.idx_to_word[l.item()]\n",
    "        res.append(w)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-sociology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('typewriter': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd003343bff8ea9174e4e18dc33629d8cc7123b4a33b966bf5614ca716c9ad2f2e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
